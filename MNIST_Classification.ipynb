{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the MNIST dataset.\n",
    "train_data = MNIST('mnist', train = True, download=True, transform= ToTensor())\n",
    "test_data = MNIST('mnist', train= False, download=True, transform= ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "for image, label in train_data:\n",
    "    # print(image.shape)\n",
    "    x = image.ravel()\n",
    "    y = label\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "for image, label in test_data:\n",
    "    x = image.reshape(-1)\n",
    "    y = label\n",
    "    x_test.append(x)\n",
    "    y_test.append(y)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RandomForestClassifier()\n",
    "RF_model.fit(x_train, y_train)\n",
    "RF_predicted = RF_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats of RF based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of RF 0.971\n",
      "\n",
      "Precision score of RF 0.970873752323483\n",
      "\n",
      "Recall score of RF 0.9707723113121738\n",
      "\n",
      "F1 score of RF 0.9708061462518804\n",
      "\n",
      "Confusion matrix of RF\n",
      " [[ 970    0    0    0    0    2    3    1    4    0]\n",
      " [   0 1122    4    3    0    1    3    0    1    1]\n",
      " [   5    0 1001    6    2    1    2    8    7    0]\n",
      " [   0    0    7  977    1    4    0   10    9    2]\n",
      " [   1    0    0    0  958    0    5    0    3   15]\n",
      " [   2    1    1   13    3  861    3    1    6    1]\n",
      " [   5    3    1    0    4    4  937    0    4    0]\n",
      " [   1    3   18    2    0    0    0  993    1   10]\n",
      " [   4    0    4    8    6    5    5    5  930    7]\n",
      " [   7    5    2   11    6    3    2    5    7  961]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test,RF_predicted)\n",
    "precision = precision_score(y_test,RF_predicted, average=\"macro\")\n",
    "recall = recall_score(y_test, RF_predicted, average=\"macro\")\n",
    "f1 = f1_score(y_test,RF_predicted, average=\"macro\")\n",
    "conf = confusion_matrix(y_test,RF_predicted)\n",
    "\n",
    "print(\"Accuracy score of RF\", accuracy, end=\"\\n\\n\")\n",
    "print(\"Precision score of RF\", precision,  end=\"\\n\\n\")\n",
    "print(\"Recall score of RF\",recall,  end=\"\\n\\n\")\n",
    "print(\"F1 score of RF\", f1,  end=\"\\n\\n\")\n",
    "print(\"Confusion matrix of RF\\n\", conf,  end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_model = DecisionTreeClassifier()\n",
    "DT_model.fit(x_train, y_train)\n",
    "DT_predicted = DT_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats of DT based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of DT 0.8741\n",
      "\n",
      "Precision score of DT 0.8726869149994633\n",
      "\n",
      "Recall score of DT 0.8725292518893152\n",
      "\n",
      "F1 score of DT 0.8725382334415371\n",
      "\n",
      "Confusion matrix of DT\n",
      " [[ 910    1   10    7    6   11   18    3    6    8]\n",
      " [   2 1085    7    7    2    6    6    5   13    2]\n",
      " [  12   12  876   37   16    9   17   23   23    7]\n",
      " [  11    2   27  861    6   47    4    8   25   19]\n",
      " [   7    4    6    5  853   10   23   10   18   46]\n",
      " [  18   10    4   38    8  742   22    6   29   15]\n",
      " [  16    6   11    9   15   25  838    1   31    6]\n",
      " [   4   12   26   17    6    2    1  927    9   24]\n",
      " [   9    8   25   43   16   24   16   10  792   31]\n",
      " [  13    1    7   22   43   12    6   21   27  857]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test,DT_predicted)\n",
    "precision = precision_score(y_test,DT_predicted, average=\"macro\")\n",
    "recall = recall_score(y_test, DT_predicted, average=\"macro\")\n",
    "f1 = f1_score(y_test,DT_predicted, average=\"macro\")\n",
    "conf = confusion_matrix(y_test,DT_predicted)\n",
    "\n",
    "print(\"Accuracy score of DT\", accuracy, end=\"\\n\\n\")\n",
    "print(\"Precision score of DT\", precision,  end=\"\\n\\n\")\n",
    "print(\"Recall score of DT\",recall,  end=\"\\n\\n\")\n",
    "print(\"F1 score of DT\", f1,  end=\"\\n\\n\")\n",
    "print(\"Confusion matrix of DT\\n\", conf,  end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanutiwari/miniconda3/envs/Oasis/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "Log_reg = LogisticRegression()\n",
    "Log_reg.fit(x_train, y_train)\n",
    "LR_predicted = Log_reg.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats for Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of LR 0.9256\n",
      "\n",
      "Precision score of LR 0.9247087190360826\n",
      "\n",
      "Recall score of LR 0.9245122318683123\n",
      "\n",
      "F1 score of LR 0.9245116077820696\n",
      "\n",
      "Confusion matrix of LR\n",
      " [[ 959    0    0    3    1    7    5    4    1    0]\n",
      " [   0 1112    4    2    0    2    3    2   10    0]\n",
      " [   6    9  928   16    8    4   15    7   35    4]\n",
      " [   4    1   17  921    0   23    4   11   23    6]\n",
      " [   1    1    7    4  914    0   10    4   10   31]\n",
      " [  10    2    3   37    8  779   14    5   29    5]\n",
      " [   9    3    7    3    8   15  910    2    1    0]\n",
      " [   1    9   23    6    7    1    0  950    2   29]\n",
      " [   9   10    8   26    8   26   12    7  857   11]\n",
      " [   9    8    0   11   23    6    0   19    7  926]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test,LR_predicted)\n",
    "precision = precision_score(y_test,LR_predicted, average=\"macro\")\n",
    "recall = recall_score(y_test, LR_predicted, average=\"macro\")\n",
    "f1 = f1_score(y_test,LR_predicted, average=\"macro\")\n",
    "conf = confusion_matrix(y_test,LR_predicted)\n",
    "\n",
    "print(\"Accuracy score of LR\", accuracy, end=\"\\n\\n\")\n",
    "print(\"Precision score of LR\", precision,  end=\"\\n\\n\")\n",
    "print(\"Recall score of LR\",recall,  end=\"\\n\\n\")\n",
    "print(\"F1 score of LR\", f1,  end=\"\\n\\n\")\n",
    "print(\"Confusion matrix of LR\\n\", conf,  end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison\n",
    "\n",
    "| Model                | Accuracy Score | Precision Score | Recall Score | F1 Score |\n",
    "|----------------------|----------------|-----------------|--------------|----------|\n",
    "| Random Forest        | 0.9710         | 0.9700          | 0.9707       | 0.9708   |\n",
    "| Decision Tree        | 0.8741         | 0.8726          | 0.8725       | 0.8725   |\n",
    "| Logistic Regression  | 0.9256         | 0.9247          | 0.9245       | 0.9245   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Oasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
